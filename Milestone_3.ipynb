{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 - training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "Using the feature extraction from the previous chapter, compute features for all videos in the training set and save them in HDF5 files. Make sure the features for genuine and Deepfake videos are saved in separate folders, so you can know if the feature comes from original genuine video or Deepfake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "Now you can compute a feature vector for a single image, which is the cropped face from a video frame.\n",
    "The goal now is, for each video and for each frame of the video, to detect the face in the video, compute the features for that face, and save the resulted feature on disk in HDF5 file. You should have **one HDF5 file for each video**. The **file will contain a matrix with the number of rows equal to the number of frames in that video and the number of columns equal to the number of features you compute for a single face**.\n",
    "The **HDF5 files should be saved in the same directory structure that the video database has, but instead of videos you will have HDF5 files with features**.\n",
    "To loop through the videos inside a directory, you can use standard python routines for recursively traversing the directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fulfill above task first recursive function of this project can be updated with feature calculation and HDF5 file saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import imutils\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner\n",
    "import math\n",
    "import h5py\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hell(hist_1, hist_2):\n",
    "    return cv2.compareHist(hist_1, hist_2, cv2.HISTCMP_BHATTACHARYYA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_hist(frame, bins_no = 100):\n",
    "    return cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_hist(image, num_bins = 100):\n",
    "    hist, bins = np.histogram(image.ravel(), num_bins, [0,256], density=True)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqr(hist_1, hist_2):\n",
    "    return cv2.compareHist(hist_1, hist_2, cv2.HISTCMP_CHISQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(image_1, image_2):\n",
    "    image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY) \n",
    "    res = cv2.matchTemplate(image_2, cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY),\n",
    "                                cv2.TM_CCOEFF_NORMED)   # match operation\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(image_1, image_2):\n",
    "    err = np.sum((image_1.astype(\"float\") - image_2.astype(\"float\"))**2)\n",
    "    err /= float(image_1.shape[0]*image_2.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurred_frame(image, kernel_size = 3, sigma_x = 5, sigma_y = 0.5):\n",
    "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma_x, sigma_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(image_1, image_2, mse): \n",
    "    if(mse == 0): \n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse)) \n",
    "    return psnr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_calc(image):\n",
    "    blurred_image = blurred_frame(image);\n",
    "    hist_0 = np_hist(image)\n",
    "    hist_1 = cv_hist(image);\n",
    "    hist_2 = cv_hist(blurred_image);\n",
    "    chi = chi_sqr(hist_1, hist_2);\n",
    "    hellinger = hell(hist_1, hist_2);\n",
    "    err = mse(image, blurred_image);\n",
    "    peak = psnr(image, blurred_image, err);\n",
    "    template_found = template(image, blurred_image);\n",
    "    (score, diff) = ssim(image, blurred_image, full = True, multichannel=True);\n",
    "    return np.concatenate([[score], [peak], [err], [hellinger], [chi], hist_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_feat = []\n",
    "f = 1\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')   # load the pre-trained model\n",
    "pat = os.getcwd()\n",
    "path = pat    # making global copy because glob was changing the path in jupyther (no such problems stationary)\n",
    "# normalisation of faces dimensions - declaration of normalisators\n",
    "predictor = dlib.shape_predictor(path + '/shape_predictor_68_face_landmarks.dat')\n",
    "fa = FaceAligner(predictor, desiredFaceWidth = 200)\n",
    "\n",
    "def getting_features(pat):\n",
    "    ''' function iterates recurively through direcory files and\n",
    "    saves the frames of encountered videos to three folders\n",
    "    1 - oryginal videos, 2 - lower quality deep fakes, \n",
    "    3 - higher quality deep fakes '''\n",
    "\n",
    "    for filename in glob.iglob(pat+'/*',\n",
    "                        recursive = True): \t\t# 1 (from Workflow list)\n",
    "        if 'avi' in filename and 'mgwt0' in filename:\n",
    "            cap = cv2.VideoCapture(filename)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            print(filename, total_frames)\n",
    "            for fno in range(0, total_frames):\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, fno)\n",
    "                image, frame = cap.read()\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                global classifier\n",
    "                global predictor\n",
    "                global fa\n",
    "                bboxes = classifier.detectMultiScale(frame)\n",
    "                for box in bboxes:\n",
    "                    # extract\n",
    "                    x, y, w, h = box\n",
    "                    rect = dlib.rectangle(x, y, w + x, h + y)   # normalisation of each frame\n",
    "                    cropped_img = fa.align(frame, gray, rect)\n",
    "                    feat = feat_calc(cropped_img)\n",
    "                    global store_feat\n",
    "                    store_feat.append(feat)\n",
    "            # converse to numpy array with n rows\n",
    "            features_save = np.stack(store_feat)\n",
    "            store_feat = []\n",
    "            with h5py.File(filename.replace('avi', 'h5'), 'w') as hf: \n",
    "                Xset = hf.create_dataset(name = 'features', data = features_save)\n",
    "            getting_features(filename)\t\t\t# recursion\n",
    "        else:\n",
    "            getting_features(filename)\t\t\t# recursion if folder is not the desired one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ricz/Desktop/Manning_books/Deep_fake/VidTIMIT/VidTIMIT/mgwt0/sx369.avi 83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1b639a59a06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f48d5db5421c>\u001b[0m in \u001b[0;36mgetting_features\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion if folder is not the desired one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f48d5db5421c>\u001b[0m in \u001b[0;36mgetting_features\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion if folder is not the desired one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f48d5db5421c>\u001b[0m in \u001b[0;36mgetting_features\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mgetting_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# recursion if folder is not the desired one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f48d5db5421c>\u001b[0m in \u001b[0;36mgetting_features\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "getting_features(pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Few Numpy methods of creating matrix from list of arrays with time changes connected with its usage:\n",
    "1. concatenate: 21 s ± 416 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "2. array: 20.8 s ± 652 ms per loop (mean ± std. dev. of 7 runs, 1 loop each\n",
    "3. vstack: 20.1 s ± 627 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "4. stack: 19.7 s ± 677 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last method, stack, was choosen because of its extraordinary good time of computation. Saved files are also experimental. They consist the template matching data wchich is not as straightforward and takes some computation time. Their efficiency will be compared to classical set of data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "Using scikit-learn train SVM classifier on the features. When reading the features from the saved HDF5 files, you need to also construct a vector with labels that has **0 label for each Deepfake features and 1 label for each genuine feature**. You can use linear SVM and play with different parameters of this classifier and study their impact on the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow\n",
    "\n",
    "1. **Split the set of videos into two sets**: training and testing. There are different ways to do it but the split of **80% of data for training and 20% for testing** is the common one. You can use train_test_split() function from sklearn.model_selection.\n",
    "2. Be careful how you split the list of videos into 80% for training and 20% for testing. You need to make sure that 80% of Deepfake videos are inside the training set and 80% of original videos are also inside the training set. Also, you need to split the videos, not their features (you have many features vectors for each video); when you evaluate later, you will need to compute one prediction score per test video, which means all features from that videos must be inside the test set. **You must always evaluate your trained classification model on the features that you did not use for training**.\n",
    "3. In a loop through all original and deepfake videos (use Python’s Glob to loop through folders), for each video compute features for all frames (loop through frames with OpenCV) in the video and save the features in HDF5 files. One HDF5 files should correspond to one video and should contain the feature matrix of N x M, where N is the number of frames in that video and M is the number of features you computed for on frame, so each row is a feature vector for one frame of the video.\n",
    "4. Once all features are computed, **focus on the training set of videos**. Loop through the stored HDF5 files (use the same Glob library) of the training set, read HDF5 files and **combine all the features vectors in one numpy array, where rows are feature vectors from all videos. In the same time, create a separate array of integer labels, which would have 0 label for the feature vector corresponding to Deepfake frame and label 1 corresponding to original frame.** In the end, you should have two arrays: 1) array of features extracted from all frames of all videos and 2) array of labels of the same length, where you store which feature is from fake video and which is from the original video.\n",
    "5. Train SVM classifier of scikit-learn on the features and labels from the training set. This trained classifier will be used in the next milestone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of videos in data directory:  1070\n",
      "\n",
      "oryginal folders and their number: \n",
      " ['fadg0', 'faks0', 'fcmh0', 'mdld0', 'mtas1', 'mrgg0', 'mjar0', 'fcmr0', 'mdbb0', 'mtmr0', 'mdab0', 'mccs0', 'mstk0', 'msjs1', 'fedw0', 'mrcz0', 'fram1', 'fdrd1', 'fdms0', 'mpgl0', 'mgwt0', 'fcft0', 'fdac1', 'mmdb1', 'mrjo0', 'fjwb0', 'fjas0', 'fkms0', 'mpdf0', 'mabw0', 'fjem0', 'mwbt0', 'mbdg0', 'mjsw0', 'fcrh0', 'mbjk0', 'mreb0', 'fpkt0', 'fgjd0', 'felc0', 'fjre0', 'mcem0', 'mmdm2'] 43\n",
      "\n",
      "HQ fake folders and their number: \n",
      " ['fadg0', 'faks0', 'fcmh0', 'mdld0', 'mrgg0', 'mjar0', 'mdbb0', 'mdab0', 'mccs0', 'mstk0', 'msjs1', 'fedw0', 'mrcz0', 'fram1', 'fdrd1', 'mpgl0', 'mgwt0', 'fcft0', 'fdac1', 'mmdb1', 'mrjo0', 'fjwb0', 'fjas0', 'fkms0', 'mpdf0', 'fjem0', 'mwbt0', 'mjsw0', 'felc0', 'fjre0', 'mcem0', 'mmdm2'] 32\n",
      "\n",
      "common part of both sets: \n",
      " ['fjre0', 'mmdb1', 'mccs0', 'fjas0', 'fedw0', 'mwbt0', 'mmdm2', 'felc0', 'mrcz0', 'mcem0', 'mdbb0', 'fcft0', 'faks0', 'mpdf0', 'fdac1', 'mdld0', 'fjwb0', 'mpgl0', 'fdrd1', 'fram1', 'mrgg0', 'fcmh0', 'fkms0', 'fadg0', 'msjs1', 'mgwt0', 'mstk0', 'mrjo0', 'mjsw0', 'mjar0', 'mdab0', 'fjem0'] 32\n"
     ]
    }
   ],
   "source": [
    "f = 1\n",
    "pat = os.getcwd()    # getting the full path to actual folder\n",
    "path = pat           # copy of path because in upyther recursion changes the oryginal\n",
    "ct = 0\n",
    "origin_folders = []\n",
    "fake_folders = []\n",
    "def getting_data(pat, sample_rate, count):\n",
    "    ''' function iterates recurively through direcory files and\n",
    "    saves the frames of encountered videos to three folders\n",
    "    1 - oryginal videos, 2 - lower quality deep fakes, \n",
    "    3 - higher quality deep fakes'''\n",
    "    for filename in glob.iglob(pat+'/*',\n",
    "                        recursive = True): \t\t# 1 (from Workflow list)\n",
    "        if 'avi' in filename:\t\t\t\t\t# 2 iteration over every frame in repository as desired in 2. of work flow\n",
    "            global ct\n",
    "            global common_folders\n",
    "            global fake_folders\n",
    "            if not pat[-5:] in origin_folders:\n",
    "                origin_folders.append(pat[-5:])\n",
    "            if 'higher' in filename:\n",
    "                if not pat[-5:] in fake_folders:\n",
    "                    fake_folders.append(pat[-5:])\n",
    "            ct += 1\n",
    "            getting_data(filename, 1, count + 1)\n",
    "        else:\n",
    "            getting_data(filename, 1, 0)\t\t\t# recursion if folder is not the desired one\n",
    "getting_data(path, 1, 0)  \n",
    "\n",
    "print('\\nnumber of videos in data directory: ', ct)\n",
    "print('\\noryginal folders and their number: \\n', origin_folders, len(origin_folders))\n",
    "print('\\nHQ fake folders and their number: \\n', fake_folders, len(fake_folders))\n",
    "list_1 = set(origin_folders)\n",
    "intersect = list_1.intersection(fake_folders)\n",
    "print('\\ncommon part of both sets: \\n', list(intersect), len(intersect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above script is a helper that once more checks the form of data directory. In oryginal videos are 110 videos without deepfake child. Intersect list provides the common folders of deep fake and oryginal videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of videos in data directory:  1070\n",
      "\n",
      "oryginal folders and their number: \n",
      " ['fadg0', 'faks0', 'fcmh0', 'mdld0', 'mtas1', 'mrgg0', 'mjar0', 'fcmr0', 'mdbb0', 'mtmr0', 'mdab0', 'mccs0', 'mstk0', 'msjs1', 'fedw0', 'mrcz0', 'fram1', 'fdrd1', 'fdms0', 'mpgl0', 'mgwt0', 'fcft0', 'fdac1', 'mmdb1', 'mrjo0', 'fjwb0', 'fjas0', 'fkms0', 'mpdf0', 'mabw0', 'fjem0', 'mwbt0', 'mbdg0', 'mjsw0', 'fcrh0', 'mbjk0', 'mreb0', 'fpkt0', 'fgjd0', 'felc0', 'fjre0', 'mcem0', 'mmdm2'] 43\n",
      "\n",
      "HQ fake folders and their number: \n",
      " ['fadg0', 'faks0', 'fcmh0', 'mdld0', 'mrgg0', 'mjar0', 'mdbb0', 'mdab0', 'mccs0', 'mstk0', 'msjs1', 'fedw0', 'mrcz0', 'fram1', 'fdrd1', 'mpgl0', 'mgwt0', 'fcft0', 'fdac1', 'mmdb1', 'mrjo0', 'fjwb0', 'fjas0', 'fkms0', 'mpdf0', 'fjem0', 'mwbt0', 'mjsw0', 'felc0', 'fjre0', 'mcem0', 'mmdm2'] 32\n",
      "\n",
      "common part of both sets: \n",
      " ['fjre0', 'mmdb1', 'mccs0', 'fjas0', 'fedw0', 'mwbt0', 'mmdm2', 'felc0', 'mrcz0', 'mcem0', 'mdbb0', 'fcft0', 'faks0', 'mpdf0', 'fdac1', 'mdld0', 'fjwb0', 'mpgl0', 'fdrd1', 'fram1', 'mrgg0', 'fcmh0', 'fkms0', 'fadg0', 'msjs1', 'mgwt0', 'mstk0', 'mrjo0', 'mjsw0', 'mjar0', 'mdab0', 'fjem0'] 32\n"
     ]
    }
   ],
   "source": [
    "pat = os.getcwd()    # getting the full path to actual folder\n",
    "path = pat           # copy of path because in upyther recursion changes the oryginal\n",
    "ct = 0\n",
    "origin_folders = []\n",
    "fake_folders = []\n",
    "def getting_data(pat, sample_rate, count):\n",
    "    ''' function iterates recurively through direcory files and\n",
    "    saves the frames of encountered videos to three folders\n",
    "    1 - oryginal videos, 2 - lower quality deep fakes, \n",
    "    3 - higher quality deep fakes'''\n",
    "    global ct\n",
    "    global common_folders\n",
    "    global fake_folders\n",
    "    for filename in glob.iglob(pat+'/*',\n",
    "                        recursive = True): \t\t# 1 (from Workflow list)\n",
    "        if 'avi' in filename:\t\t\t\t\t# 2 iteration over every frame in repository as desired in 2. of work flow\n",
    "            if not pat[-5:] in origin_folders:\n",
    "                origin_folders.append(pat[-5:])\n",
    "            if 'higher' in filename:\n",
    "                if not pat[-5:] in fake_folders:\n",
    "                    fake_folders.append(pat[-5:])\n",
    "            ct += 1\n",
    "            getting_data(filename, 1, count + 1)\n",
    "        else:\n",
    "            getting_data(filename, 1, 0)\t\t\t# recursion if folder is not the desired one\n",
    "getting_data(path, 1, 0)  \n",
    "\n",
    "print('\\nnumber of videos in data directory: ', ct)\n",
    "print('\\noryginal folders and their number: \\n', origin_folders, len(origin_folders))\n",
    "print('\\nHQ fake folders and their number: \\n', fake_folders, len(fake_folders))\n",
    "list_1 = set(origin_folders)\n",
    "intersect = list_1.intersection(fake_folders)\n",
    "print('\\ncommon part of both sets: \\n', list(intersect), len(intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = os.getcwd()    # getting the full path to actual folder\n",
    "path = pat           # copy of path because in upyther recursion changes the oryginal\n",
    "ct = 1\n",
    "training_videos = []\n",
    "eval_videos = []\n",
    "\n",
    "\n",
    "def getting_data(pat):\n",
    "    global ct\n",
    "    global training_videos\n",
    "    global eval_videos\n",
    "    for filename in glob.iglob(pat+'/*',\n",
    "                        recursive = True): \t\t# 1 (from Workflow list)\n",
    "        if 'h5' in filename:\t\t\t\t\t# 2 iteration over every frame in repository as desired in 2. of work flow\n",
    "            if any(name in filename for name in intersect):\n",
    "                if ct <= 8:\n",
    "                    ct += 1\n",
    "                    training_videos.append(filename)\n",
    "                elif ct > 8 and ct < 10:\n",
    "                    ct += 1\n",
    "                    eval_videos.append(filename)\n",
    "                elif ct == 10:\n",
    "                    ct = 1\n",
    "                    eval_videos.append(filename)\n",
    "            getting_data(filename)\n",
    "        else:\n",
    "            getting_data(filename)\t\t\t# recursion if folder is not the desired one\n",
    "getting_data(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frames and length of label vector:  86078\n",
      "shape of combined h5 files:  (86078, 105)\n",
      "number of fake/oryginal videos:  512 256\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "labels = []\n",
    "ct = 0\n",
    "ct_1 = 0\n",
    "for i in training_videos:\n",
    "    hf = h5py.File(i, 'r')\n",
    "    data = hf.get('features')[()]\n",
    "    all_features.append(data)\n",
    "    (x, y) = data.shape\n",
    "    if 'Deepfake' in i:\n",
    "        ct += 1\n",
    "        for j in range(0, x):\n",
    "            labels.append(0)\n",
    "    else:\n",
    "        ct_1 += 1\n",
    "        for j in range(0, x):\n",
    "            labels.append(1)\n",
    "all_features = np.concatenate(all_features)\n",
    "print('number of frames and length of label vector: ', len(labels))\n",
    "print('shape of combined h5 files: ', all_features.shape)\n",
    "print('number of fake/oryginal videos: ', ct, ct_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frames and length of label vector:  20411\n",
      "shape of combined h5 files:  (20411, 105)\n",
      "number of fake/oryginal videos:  128 64\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "test_labels = []\n",
    "ct = 0\n",
    "ct_1 = 0\n",
    "for i in eval_videos:\n",
    "    hf = h5py.File(i, 'r')\n",
    "    data = hf.get('features')[()]\n",
    "    test_features.append(data)\n",
    "    (x, y) = data.shape\n",
    "    if 'Deepfake' in i:\n",
    "        ct += 1\n",
    "        for j in range(0, x):\n",
    "            test_labels.append(0)\n",
    "    else:\n",
    "        ct_1 += 1\n",
    "        for j in range(0, x):\n",
    "            test_labels.append(1)\n",
    "test_features = np.concatenate(test_features)\n",
    "print('number of frames and length of label vector: ', len(test_labels))\n",
    "print('shape of combined h5 files: ', test_features.shape)\n",
    "print('number of fake/oryginal videos: ', ct, ct_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st method\n",
    "X = all_features\n",
    "y = np.array(labels)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of errors:  4629\n"
     ]
    }
   ],
   "source": [
    "data = test_features\n",
    "pred = clf.predict(data)\n",
    "ct = 0\n",
    "for i in range(0, len(pred)):\n",
    "    if pred[i] != test_labels[i]:\n",
    "        ct += 1\n",
    "print('No. of errors: ', ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricz/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=100000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2nd method \n",
    "X_test = test_features\n",
    "y_test = test_labels\n",
    "X = all_features\n",
    "y = np.array(labels)\n",
    "svclassifier = SVC(kernel = 'linear', max_iter = 100000)\n",
    "svclassifier.fit(X, y)\n",
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5812 7706]\n",
      " [5036 1857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.43      0.48     13518\n",
      "           1       0.19      0.27      0.23      6893\n",
      "\n",
      "    accuracy                           0.38     20411\n",
      "   macro avg       0.36      0.35      0.35     20411\n",
      "weighted avg       0.42      0.38      0.39     20411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "svclassifier_1 = SVC(kernel = 'poly', degree = 8, max_iter = 100000)\n",
    "svclassifier_1.fit(X, y)\n",
    "y_pred_1 = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_1))\n",
    "print(classification_report(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f43bda09af40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Gaussian kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvclassifier_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvclassifier_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Gaussian kernel\n",
    "svclassifier_2 = SVC(kernel = 'rbf')\n",
    "svclassifier_2.fit(X, y)\n",
    "y_pred_2 = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_2))\n",
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid kernel\n",
    "svclassifier_3 = SVC(kernel = 'sigmoid')\n",
    "svclassifier_3.fit(X, y)\n",
    "y_pred_3 = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_3))\n",
    "print(classification_report(y_test, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
